{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be615dfc-63cc-4c5c-86cc-41c270b85259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to metadata file and dataset\n",
    "metadata_path = temp # TODO Update this.\n",
    "audio_dir =  temp # TODO Update this.\n",
    "output_dir =  temp # TODO Update this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b2b440-d52f-4de2-bc9c-25924ddd42fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create output directory if not exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "df = pd.read_csv(metadata_path)\n",
    "\n",
    "# Keep only the first 6,000 files\n",
    "df = df.iloc[:6000]\n",
    "\n",
    "# Map labels: 'bona-fide' → 0 (real), 'spoof' → 1 (fake)\n",
    "df[\"label\"] = df[\"label\"].map({\"bona-fide\": 0, \"spoof\": 1})\n",
    "\n",
    "# Save updated metadata\n",
    "df.to_csv(os.path.join(output_dir, \"mel_labels.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb05563e-be3b-43b6-85e0-cb0e5f5cffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mel_spectrogram(audio_path, save_path):\n",
    "    \"\"\"Converts a .wav file to a Mel spectrogram and saves it as an image.\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(audio_path, sr=16000)  # Load audio\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to decibels\n",
    "\n",
    "        # Save as an image\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        librosa.display.specshow(mel_spec_db, sr=sr, x_axis=\"time\", y_axis=\"mel\")\n",
    "        plt.axis(\"off\")  # No axes for clean image\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\", pad_inches=0)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "# Process first 6,000 files\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    file_name = row[\"file\"]  # File name from metadata\n",
    "    file_path = os.path.join(audio_dir, file_name)  # Full path\n",
    "    save_path = os.path.join(output_dir, f\"{file_name}.png\")  # Save as PNG\n",
    "    save_mel_spectrogram(file_path, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e2dd16-6b04-45e2-ab12-288fd239aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the metadata CSV\n",
    "df = pd.read_csv(os.path.join(output_dir, \"mel_labels.csv\"))\n",
    "\n",
    "# Split into train (80%), validation (10%), and test (10%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42)\n",
    "\n",
    "# Save new CSV files\n",
    "train_df.to_csv(os.path.join(output_dir, \"train.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(output_dir, \"val.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(output_dir, \"test.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d32b42e-8484-4920-b3b3-6c3814fab534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize for Res2Net\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize\n",
    "])\n",
    "\n",
    "class MelSpectrogramDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, self.data.iloc[idx, 0] + \".png\")\n",
    "        image = Image.open(img_name).convert(\"RGB\")  # Convert grayscale to RGB\n",
    "        label = self.data.iloc[idx, 2]  # Label (0 for real, 1 for fake)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MelSpectrogramDataset(os.path.join(output_dir, \"train.csv\"), output_dir, transform)\n",
    "val_dataset = MelSpectrogramDataset(os.path.join(output_dir, \"val.csv\"), output_dir, transform)\n",
    "test_dataset = MelSpectrogramDataset(os.path.join(output_dir, \"test.csv\"), output_dir, transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10982437-0088-4718-93a2-1bbfb3e9065e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 300\n",
      "Number of validation batches: 38\n",
      "Number of test batches: 38\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ebe0cd-fa46-4997-9e9d-1cbc47007c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class Res2NetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Res2NetClassifier, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=False)  # Using ResNet50 as a base\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)  # Modify FC layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = Res2NetClassifier().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b094d8ee-ff67-4335-a1a0-944cb2fd107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "317c79f1-7002-4e0e-bcce-059a77fdf0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        \n",
    "        # for images, labels in train_loader:\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct, val_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "526fdaae-65f0-4fa1-90b9-3dc45041ca79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|█████████████████████████████████████████████████████████████████| 300/300 [02:45<00:00,  1.81batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 31.9885, Accuracy: 0.9621\n",
      "Validation Accuracy: 0.9783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|█████████████████████████████████████████████████████████████████| 300/300 [02:35<00:00,  1.92batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 62.7321, Accuracy: 0.9340\n",
      "Validation Accuracy: 0.9700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|█████████████████████████████████████████████████████████████████| 300/300 [02:35<00:00,  1.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 26.0170, Accuracy: 0.9729\n",
      "Validation Accuracy: 0.9733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|█████████████████████████████████████████████████████████████████| 300/300 [02:35<00:00,  1.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 23.2363, Accuracy: 0.9744\n",
      "Validation Accuracy: 0.9767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|█████████████████████████████████████████████████████████████████| 300/300 [03:16<00:00,  1.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 16.5695, Accuracy: 0.9844\n",
      "Validation Accuracy: 0.9867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|█████████████████████████████████████████████████████████████████| 300/300 [02:39<00:00,  1.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 15.7357, Accuracy: 0.9827\n",
      "Validation Accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|█████████████████████████████████████████████████████████████████| 300/300 [02:34<00:00,  1.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 12.4760, Accuracy: 0.9892\n",
      "Validation Accuracy: 0.9700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|█████████████████████████████████████████████████████████████████| 300/300 [02:36<00:00,  1.92batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 13.4184, Accuracy: 0.9871\n",
      "Validation Accuracy: 0.9767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|█████████████████████████████████████████████████████████████████| 300/300 [02:35<00:00,  1.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 9.3995, Accuracy: 0.9885\n",
      "Validation Accuracy: 0.9733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|████████████████████████████████████████████████████████████████| 300/300 [02:36<00:00,  1.92batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 13.0961, Accuracy: 0.9852\n",
      "Validation Accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a28f402d-b06d-4be3-936b-300f70eaa16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress Bar: 100%|█████████████████████████████████████████████████████████████████| 38/38 [00:18<00:00,  2.07batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # for images, labels in test_loader:\n",
    "        for images, labels in tqdm(test_loader, desc=\"Progress Bar\", unit=\"batch\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    test_acc = correct / total\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d3e3c-ca3e-4f6d-aafa-f5d6fc959bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after training\n",
    "save_dir=temp # TODO add path to save model.\n",
    "model_name = \"deepfake_audio_detector_.pth\"\n",
    "save_path = os.path.join(save_dir, model_name)\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7c1c2-40f4-41df-a50c-fa30e3455b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
